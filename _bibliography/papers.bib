---

@inproceedings{auto-gen-contrast-set,
  abbr={BlackboxNLP},
  author = "{Chuanrong Li*, Lin Shengshuo*, Leo Z. Liu*, Xinyi Wu*, Xuhui Zhou* and Shane Steinert-Threlkeld}",
  title = "Linguistically-Informed Transformations (LIT): A Method for Automatically Generating Contrast Sets",
  abstract={Although large-scale pretrained language models, such as BERT and RoBERTa, have achieved superhuman performance on in-distribution test sets, their performance suffers on out-of-distribution test sets (e.g., on contrast sets). Building contrast sets often re-quires human-expert annotation, which is expensive and hard to create on a large scale. In this work, we propose a Linguistically-Informed Transformation (LIT) method to automatically generate contrast sets, which enables practitioners to explore linguistic phenomena of interests as well as compose different phenomena. Experimenting with our method on SNLI and MNLI shows that current pretrained language models, although being claimed to contain sufficient linguistic knowledge, struggle on our automatically generated contrast sets. Furthermore, we improve models' performance on the contrast sets by apply-ing LIT to augment the training data, without affecting performance on the original data.},
  booktitle = "Proc. of EMNLP BlackboxNLP Workshop (Poster)",
  state="accepted",
  year = "2020",
  pdf={https://arxiv.org/pdf/2010.08580.pdf},
  note = {(author sorted alphabetically)},
}

@inproceedings{dynamic-probing,
  abbr={arXiv},
  author = "{Leo Z. Liu*, Yizhong Wang*, Jungo Kasai, Hannaneh Hajishirzi, and Noah A. Smith}",
  title = "Probing Across Time: What Does RoBERTa Know and When?",
  abstract={Models of language trained on very large corpora have been demonstrated useful for NLP. As fixed artifacts, they have become the object of intense study, with many researchers ``probing'' the extent to which linguistic abstractions, factual and commonsense knowledge, and reasoning abilities, they acquire and readily demonstrate.  Building on this line of work, we consider a new question: for types of knowledge a language model learns, when during (pre)training are they acquired? We plot probing performance across iterations, using RoBERTa as a case study. Among our findings:  linguistic knowledge is acquired fast, stably, and robustly across domains.  Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster. },
  booktitle = "arXiv",
  pdf={https://arxiv.org/pdf/2104.07885.pdf},
  state="accepted",
  note = {},
  year = "2021"
}
