---

@inproceedings{li-etal-2020-linguistically,
    abbr={BlackboxNLP  <br> @ EMNLP},
    title = "Linguistically-Informed Transformations ({LIT}): A Method for Automatically Generating Contrast Sets",
    author = "{Chuanrong Li<sup>$\alpha$</sup>, 
      Lin Shengshuo<sup>$\alpha$</sup>, 
      <font size="+1"><b>Leo Z. Liu<sup>$\alpha$</sup></b></font>, 
      Xinyi Wu<sup>$\alpha$</sup>, 
      Xuhui Zhou<sup>$\alpha$</sup>, and
      Shane Steinert-Threlkeld}",
    editor = "Alishahi, Afra  and
      Belinkov, Yonatan  and
      Chrupa{\l}a, Grzegorz  and
      Hupkes, Dieuwke  and
      Pinter, Yuval  and
      Sajjad, Hassan",
    booktitle = "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.blackboxnlp-1.12",
    pdf = "https://aclanthology.org/2020.blackboxnlp-1.12",
    arxiv = "2010.08580",
    doi = "10.18653/v1/2020.blackboxnlp-1.12",
    pages = "126--135",
    abstract = "Although large-scale pretrained language models, such as BERT and RoBERTa, have achieved superhuman performance on in-distribution test sets, their performance suffers on out-of-distribution test sets (e.g., on contrast sets). Building contrast sets often requires human-expert annotation, which is expensive and hard to create on a large scale. In this work, we propose a Linguistically-Informed Transformation (LIT) method to automatically generate contrast sets, which enables practitioners to explore linguistic phenomena of interests as well as compose different phenomena. Experimenting with our method on SNLI and MNLI shows that current pretrained language models, although being claimed to contain sufficient linguistic knowledge, struggle on our automatically generated contrast sets. Furthermore, we improve models{'} performance on the contrast sets by applying LIT to augment the training data, without affecting performance on the original data.",
}

@inproceedings{liu-etal-2021-probing-across,
    abbr={Findings <br> @ EMNLP},
    title = "Probing Across Time: What Does {R}o{BERT}a Know and When?",
    author = "{<font size="+1"><b>Leo Z. Liu<sup>*</sup></b></font>,
      Yizhong Wang<sup>*</sup>,
      Jungo Kasai,
      Hannaneh Hajishirzi, 
      Noah A. Smith}",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.71",
    pdf = "https://aclanthology.org/2021.findings-emnlp.71",
    arxiv = "2104.07885", 
    doi = "10.18653/v1/2021.findings-emnlp.71",
    pages = "820--842",
    abstract = "Models of language trained on very large corpora have been demonstrated useful for natural language processing. As fixed artifacts, they have become the object of intense study, with many researchers {``}probing{''} the extent to which they acquire and readily demonstrate linguistic abstractions, factual and commonsense knowledge, and reasoning abilities. Recent work applied several probes to intermediate training stages to observe the developmental process of a large-scale model (Chiang et al., 2020). Following this effort, we systematically answer a question: for various types of knowledge a language model learns, when during (pre)training are they acquired? Using RoBERTa as a case study, we find: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster.",
}

@inproceedings{ec-ft-position,
  abbr={EmeCom <br> @ ICLR},
  author = "{Shane Steinert-Threlkeld,
  Xuhui Zhou, 
  <font size="+1"><b>Leo Z. Liu</b></font>, and
  C. M. Downey}",
  title = "Emergent Communication Fine-tuning (EC-FT) for Pretrained Language Models",
  abstract={It has recently been argued that the currently dominant paradigm in NLP of pretraining on text-only corpora will not yield robust natural language understanding systems.  One strain of this argumentation highlights the need for grounded, goal-oriented, and interactive language learning.  In this position paper, we articulate how Emergent Communication (EC) can be used in conjunction with large pretrained language models as a `Fine-Tuning' (FT) step (hence, EC-FT) in order to provide them with supervision from such learning scenarios.  We discuss methodological issues and difficulties with making this work, and then illustrate the overall idea with a case study in unsupervised machine translation, before concluding with a discussion on the relation to multimodal pretraining.},
  booktitle = "Proc. of ICLR EmeCom Workshop (<b>Runner-up Best Paper</b>)",
  pdf = {https://openreview.net/pdf?id=SUqrM7WR7W5},
  year = "2022"
}

@inproceedings{liu-etal-2023-towards-unified,
    abbr={EMNLP},
    title = "Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model",
    author = "{<font size="+1"><b>Leo Z. Liu</b></font>, 
      Tim Dettmers,
      Xi Victoria Lin,
      Veselin Stoyanov, and
      Xian Li}",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.930",
    pdf = "https://aclanthology.org/2023.emnlp-main.930",
    arxiv = "2305.13999",
    doi = "10.18653/v1/2023.emnlp-main.930",
    pages = "15038--15061",
    abstract = "Large and sparse feed-forward layers (S-FFN) such as Mixture-of-Experts (MoE) have proven effective in scaling up Transformers model size for pretraining large language models. By only activating part of the FFN parameters conditioning on input, S-FFN improves generalization performance while keeping training and inference costs (in FLOPs) fixed. In this work, we analyzed two major design choices of S-FFN: the memory block (a.k.a. expert) size and the memory block selection method under a general conceptual framework of sparse neural memory. Using this unified framework, we compare several S-FFN architectures for language modeling and provide insights into their relative efficacy and efficiency. We found a simpler selection method {---} Avg-K that selects blocks through their mean aggregated hidden states, achieving lower perplexity in language model pretraining compared to existing MoE architectures including Switch Transformer (Fedus et al., 2021) and HashLayer (Roller et al., 2021).",
}

@inproceedings{downey-etal-2023-learning,
    abbr={MRL <br> @ EMNLP},
    title = "Learning to translate by learning to communicate",
    author = "{C.M. Downey,
      Xuhui Zhou,
      <font size="+1"><b>Leo Z. Liu</b></font>, and
      Shane Steinert-Threlkeld}",
    editor = "Ataman, Duygu",
    booktitle = "Proceedings of the 3rd Workshop on Multi-lingual Representation Learning (MRL)",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.mrl-1.17",
    pdf = "https://aclanthology.org/2023.mrl-1.17",
    arxiv = "2207.07025",
    doi = "10.18653/v1/2023.mrl-1.17",
    pages = "218--238",
    abstract = "We formulate and test a technique to use Emergent Communication (EC) with a pre-trained multilingual model to improve on modern Unsupervised NMT systems, especially for low-resource languages. It has been argued that the current dominant paradigm in NLP of pre-training on text-only corpora will not yield robust natural language understanding systems, and the need for grounded, goal-oriented, and interactive language learning has been high lighted. In our approach, we embed a multilingual model (mBART, Liu et al., 2020) into an EC image-reference game, in which the model is incentivized to use multilingual generations to accomplish a vision-grounded task. The hypothesis is that this will align multiple languages to a shared task space. We present two variants of EC Fine-Tuning (Steinert-Threlkeld et al., 2022), one of which outperforms a backtranslation-only baseline in all four languages investigated, including the low-resource language Nepali.",
}

@inproceedings{xie2024openagents,
abbr={LLM Agents <br> @ ICLR <br><br> COLM <br> under review},
title={OpenAgents: An Open Platform for Language Agents in the Wild},
author={{Tianbao Xie<sup>*</sup>}, {Fan Zhou<sup>*</sup>} and {Zhoujun Cheng<sup>*</sup>, Peng Shi<sup>*</sup>}, {Luoxuan Weng<sup>*</sup>} and {Yitao Liu<sup>*</sup>}, {Toh Jing Hua} and {Junning Zhao, Qian Liu}, {Che Liu} and {<font size="+1"><b>Leo Z. Liu</b></font>, Yiheng Xu}, {Hongjin Su} and {Dongchan Shin}, {Caiming Xiong} and {Tao Yu}},
booktitle={ICLR 2024 Workshop on Large Language Model (LLM) Agents},
year="2024",
url={https://openreview.net/forum?id=m2WwROxCcB},
pdf ={https://openreview.net/forum?id=m2WwROxCcB},
arxiv = {2310.10634},
abstract = "Language agents show potential in being capable of utilizing natural language for varied and intricate tasks in diverse environments, particularly when built upon large language models (LLMs). Current language agent frameworks aim to fa- cilitate the construction of proof-of-concept language agents while neglecting the non-expert user access to agents and paying little attention to application-level de- signs. We present OpenAgents, an open platform for using and hosting language agents in the wild of everyday life. OpenAgents includes three agents: (1) Data Agent for data analysis with Python/SQL and data tools; (2) Plugins Agent with 200+ daily API tools; (3) Web Agent for autonomous web browsing. OpenAgents enables general users to interact with agent functionalities through a web user in- terface optimized for swift responses and common failures while offering develop- ers and researchers a seamless deployment experience on local setups, providing a foundation for crafting innovative language agents and facilitating real-world evaluations. We elucidate the challenges and opportunities, aspiring to set a foun- dation for future research and development of real-world language agents.",
}

@article{CodeUpdateArena,
  author       = {{<font size="+1"><b>Zeyu Leo Liu</b></font>} and
                  Shrey Pandit and
                  Xi Ye and
                  Eunsol Choi and
                  Greg Durrett},
  abbr={arxiv},
  booktitle = {arxiv},
  title        = {CodeUpdateArena: Benchmarking Knowledge Editing on {API} Updates},
  journal      = {arxiv},
  volume       = {abs/2407.06249},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2407.06249},
  
  doi          = {10.48550/ARXIV.2407.06249},
  eprinttype    = {arXiv},
  eprint       = {2407.06249},
  timestamp    = {Mon, 12 Aug 2024 20:53:51 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2407-06249.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  pdf = {https://arxiv.org/pdf/2407.06249},
  arxiv = {2407.06249},
  abstract={Large language models (LLMs) are increasingly being used to synthesize and reason about source code. The libraries and API functions they invoke are continuously evolving, with functionality being added or changing. Yet, no prior work has studied how an LLM's knowledge about code API functions can be updated. To fill this gap, we present `CodeUpdateArena`, a benchmark for knowledge editing in the code domain. An instance in our benchmark consists of a synthetic API function update paired with a program synthesis example that uses the updated functionality; our goal is to update an LLM to be able to solve this program synthesis example \emph{without providing documentation of the update at inference time}. Compared to knowledge editing for facts, success here is more challenging: a code LLM must reason about the semantics of the modified function rather than just reproduce its syntax. Our dataset is constructed by first prompting GPT-$4$ to generate atomic and executable function updates. Then, for each update, we generate program synthesis examples whose code solutions are prone to use the update. Our benchmark covers updates of various types to 54 functions from seven diverse Python packages, with a total of 670 program synthesis examples. Our experiments show that fine-tuning open-source code LLMs (i.e., DeepSeek, CodeLlama) on documentation of a new update does not allow them to incorporate changes for problem-solving. However, prepending the same information does help, establishing that the information is present, and careful fine-tuning on examples demonstrating the update shows improvement, paving the way for better knowledge editing techniques for code.}
}

@article{ComposableCoT,
  author       = {Fangcong Yin and
                  {<font size="+1"><b>Zeyu Leo Liu</b></font>} and
                  Liu Leqi and
                  Xi Ye and
                  Greg Durrett},
  abbr={arxiv},
  booktitle = {arxiv},
  title        = {Learning Composable Chains-of-Thought},
  journal      = {In Submission to ICLR 2026},
  volume       = {abs/2505.22635},
  year         = {2025},
  
  eprinttype    = {arXiv},
  eprint       = {2505.22635},
  timestamp    = {Thu, 26 May 2025 12:00:00 +0000},
  pdf = {https://arxiv.org/pdf/2505.22635},
  arxiv = {2505.22635},
  abstract={A common approach for teaching large language models (LLMs) to reason is to train on chain-of-thought (CoT) traces of in-distribution reasoning problems, but such annotated data is costly to obtain for every problem of interest. We want reasoning models to generalize beyond their training distribution, and ideally to generalize compositionally: combine atomic reasoning skills to solve harder, unseen reasoning tasks. We take a step towards compositional generalization of reasoning skills when addressing a target compositional task that has no labeled CoT data. We find that simply training models on CoT data of atomic tasks leads to limited generalization, but minimally modifying CoT formats of constituent atomic tasks to be composable can lead to improvements. We can train “atomic CoT” models on the atomic tasks with Composable CoT data and combine them with multitask learning or model merging for better zero-shot performance on the target compositional task. Such a combined model can be further bootstrapped on a small amount of compositional data using rejection sampling fine-tuning (RFT). Results on string operations and natural language skill compositions show that training LLMs on Composable CoT outperforms multitask learning and continued fine-tuning baselines within a given training data budget.}
}

@article{ComposableCoT,
  author       = { {<font size="+1"><b>Zeyu Leo Liu</b></font>} and
                  Greg Durrett and
                  Eunsol Choi},
  abbr={arxiv},
  booktitle = {arxiv},
  title        = {PropMEND: Hypernetworks for Knowledge Propagation in LLMs},
  journal      = {In Submission to ICLR 2026},
  volume       = {abs/2506.08920},
  year         = {2025},
  
  eprinttype    = {arXiv},
  eprint       = {2506.08920},
  timestamp    = {Thu, 26 Jun 2025 12:00:00 +0000},
  pdf = {https://arxiv.org/pdf/2506.08920},
  arxiv = {2506.08920},
  abstract={Knowledge editing techniques for large language models (LLMs) can inject knowledge that is later reproducible verbatim, but they fall short on propagating that knowledge: models cannot answer questions that require reasoning with the injected knowledge. We present a hypernetwork-based approach for knowledge propagation, named PropMEND, where we meta-learn how to modify gradients of a language modeling loss to encourage injected information to propagate. Our approach extends the meta-objective of MEND [29] so that gradient updates on knowledge are transformed to enable answering multi-hop questions involving that knowledge. We show improved performance on the RippleEdit dataset, showing almost 2×accuracy on challenging multi-hop questions whose answers are not explicitly stated in the injected fact. We further introduce a new dataset, Controlled RippleEdit, to evaluate the generalization of our hypernetwork, testing knowledge propagation along relations and entities unseen during hypernetwork training. PropMEND still outperforms existing approaches in unseen entity-relation pairs, yet the performance gap decreases substantially, suggesting future work in propagating knowledge to a wide range of relations.}
}