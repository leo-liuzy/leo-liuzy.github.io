---

@inproceedings{li-etal-2020-linguistically,
    abbr={BlackboxNLP  <br> @ EMNLP},
    title = "Linguistically-Informed Transformations ({LIT}): A Method for Automatically Generating Contrast Sets",
    author = "{Chuanrong Li<sup>$\alpha$</sup>, 
      Lin Shengshuo<sup>$\alpha$</sup>, 
      <font size="+1"><b>Leo Z. Liu<sup>$\alpha$</sup></b></font>, 
      Xinyi Wu<sup>$\alpha$</sup>, 
      Xuhui Zhou<sup>$\alpha$</sup>, and
      Shane Steinert-Threlkeld}",
    editor = "Alishahi, Afra  and
      Belinkov, Yonatan  and
      Chrupa{\l}a, Grzegorz  and
      Hupkes, Dieuwke  and
      Pinter, Yuval  and
      Sajjad, Hassan",
    booktitle = "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.blackboxnlp-1.12",
    pdf = "https://aclanthology.org/2020.blackboxnlp-1.12",
    arxiv = "2010.08580",
    doi = "10.18653/v1/2020.blackboxnlp-1.12",
    pages = "126--135",
    abstract = "Although large-scale pretrained language models, such as BERT and RoBERTa, have achieved superhuman performance on in-distribution test sets, their performance suffers on out-of-distribution test sets (e.g., on contrast sets). Building contrast sets often requires human-expert annotation, which is expensive and hard to create on a large scale. In this work, we propose a Linguistically-Informed Transformation (LIT) method to automatically generate contrast sets, which enables practitioners to explore linguistic phenomena of interests as well as compose different phenomena. Experimenting with our method on SNLI and MNLI shows that current pretrained language models, although being claimed to contain sufficient linguistic knowledge, struggle on our automatically generated contrast sets. Furthermore, we improve models{'} performance on the contrast sets by applying LIT to augment the training data, without affecting performance on the original data.",
}

@inproceedings{liu-etal-2021-probing-across,
    abbr={Findings <br> @ EMNLP},
    title = "Probing Across Time: What Does {R}o{BERT}a Know and When?",
    author = "{<font size="+1"><b>Leo Z. Liu<sup>*</sup></b></font>,
      Yizhong Wang<sup>*</sup>,
      Jungo Kasai,
      Hannaneh Hajishirzi, 
      Noah A. Smith}",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.71",
    pdf = "https://aclanthology.org/2021.findings-emnlp.71",
    arxiv = "2104.07885", 
    doi = "10.18653/v1/2021.findings-emnlp.71",
    pages = "820--842",
    abstract = "Models of language trained on very large corpora have been demonstrated useful for natural language processing. As fixed artifacts, they have become the object of intense study, with many researchers {``}probing{''} the extent to which they acquire and readily demonstrate linguistic abstractions, factual and commonsense knowledge, and reasoning abilities. Recent work applied several probes to intermediate training stages to observe the developmental process of a large-scale model (Chiang et al., 2020). Following this effort, we systematically answer a question: for various types of knowledge a language model learns, when during (pre)training are they acquired? Using RoBERTa as a case study, we find: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster.",
}

@inproceedings{ec-ft-position,
  abbr={EmeCom <br> @ ICLR},
  author = "{Shane Steinert-Threlkeld,
  Xuhui Zhou, 
  <font size="+1"><b>Leo Z. Liu</b></font>, and
  C. M. Downey}",
  title = "Emergent Communication Fine-tuning (EC-FT) for Pretrained Language Models",
  abstract={It has recently been argued that the currently dominant paradigm in NLP of pretraining on text-only corpora will not yield robust natural language understanding systems.  One strain of this argumentation highlights the need for grounded, goal-oriented, and interactive language learning.  In this position paper, we articulate how Emergent Communication (EC) can be used in conjunction with large pretrained language models as a `Fine-Tuning' (FT) step (hence, EC-FT) in order to provide them with supervision from such learning scenarios.  We discuss methodological issues and difficulties with making this work, and then illustrate the overall idea with a case study in unsupervised machine translation, before concluding with a discussion on the relation to multimodal pretraining.},
  booktitle = "Proc. of ICLR EmeCom Workshop (<b>Runner-up Best Paper</b>)",
  pdf = {https://openreview.net/pdf?id=SUqrM7WR7W5},
  year = "2022"
}

@inproceedings{liu-etal-2023-towards-unified,
    abbr={EMNLP},
    title = "Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model",
    author = "{<font size="+1"><b>Leo Z. Liu</b></font>, 
      Tim Dettmers,
      Xi Victoria Lin,
      Veselin Stoyanov, and
      Xian Li}",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.930",
    pdf = "https://aclanthology.org/2023.emnlp-main.930",
    arxiv = "2305.13999",
    doi = "10.18653/v1/2023.emnlp-main.930",
    pages = "15038--15061",
    abstract = "Large and sparse feed-forward layers (S-FFN) such as Mixture-of-Experts (MoE) have proven effective in scaling up Transformers model size for pretraining large language models. By only activating part of the FFN parameters conditioning on input, S-FFN improves generalization performance while keeping training and inference costs (in FLOPs) fixed. In this work, we analyzed two major design choices of S-FFN: the memory block (a.k.a. expert) size and the memory block selection method under a general conceptual framework of sparse neural memory. Using this unified framework, we compare several S-FFN architectures for language modeling and provide insights into their relative efficacy and efficiency. We found a simpler selection method {---} Avg-K that selects blocks through their mean aggregated hidden states, achieving lower perplexity in language model pretraining compared to existing MoE architectures including Switch Transformer (Fedus et al., 2021) and HashLayer (Roller et al., 2021).",
}

@inproceedings{downey-etal-2023-learning,
    abbr={MRL <br> @ EMNLP},
    title = "Learning to translate by learning to communicate",
    author = "{C.M. Downey,
      Xuhui Zhou,
      <font size="+1"><b>Leo Z. Liu</b></font>, and
      Shane Steinert-Threlkeld}",
    editor = "Ataman, Duygu",
    booktitle = "Proceedings of the 3rd Workshop on Multi-lingual Representation Learning (MRL)",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.mrl-1.17",
    pdf = "https://aclanthology.org/2023.mrl-1.17",
    arxiv = "2207.07025",
    doi = "10.18653/v1/2023.mrl-1.17",
    pages = "218--238",
    abstract = "We formulate and test a technique to use Emergent Communication (EC) with a pre-trained multilingual model to improve on modern Unsupervised NMT systems, especially for low-resource languages. It has been argued that the current dominant paradigm in NLP of pre-training on text-only corpora will not yield robust natural language understanding systems, and the need for grounded, goal-oriented, and interactive language learning has been high lighted. In our approach, we embed a multilingual model (mBART, Liu et al., 2020) into an EC image-reference game, in which the model is incentivized to use multilingual generations to accomplish a vision-grounded task. The hypothesis is that this will align multiple languages to a shared task space. We present two variants of EC Fine-Tuning (Steinert-Threlkeld et al., 2022), one of which outperforms a backtranslation-only baseline in all four languages investigated, including the low-resource language Nepali.",
}

@inproceedings{xie2024openagents,
abbr={LLM Agents <br> @ ICLR <br><br> COLM <br> under review},
title={OpenAgents: An Open Platform for Language Agents in the Wild},
author={{Tianbao Xie<sup>*</sup>}, {Fan Zhou<sup>*</sup>} and {Zhoujun Cheng<sup>*</sup>, Peng Shi<sup>*</sup>}, {Luoxuan Weng<sup>*</sup>} and {Yitao Liu<sup>*</sup>}, {Toh Jing Hua} and {Junning Zhao, Qian Liu}, {Che Liu} and {<font size="+1"><b>Leo Z. Liu</b></font>, Yiheng Xu}, {Hongjin Su} and {Dongchan Shin}, {Caiming Xiong} and {Tao Yu}},
booktitle={ICLR 2024 Workshop on Large Language Model (LLM) Agents},
year="2024",
url={https://openreview.net/forum?id=m2WwROxCcB},
pdf ={https://openreview.net/forum?id=m2WwROxCcB},
arxiv = {2310.10634},
abstract = "Language agents show potential in being capable of utilizing natural language for varied and intricate tasks in diverse environments, particularly when built upon large language models (LLMs). Current language agent frameworks aim to fa- cilitate the construction of proof-of-concept language agents while neglecting the non-expert user access to agents and paying little attention to application-level de- signs. We present OpenAgents, an open platform for using and hosting language agents in the wild of everyday life. OpenAgents includes three agents: (1) Data Agent for data analysis with Python/SQL and data tools; (2) Plugins Agent with 200+ daily API tools; (3) Web Agent for autonomous web browsing. OpenAgents enables general users to interact with agent functionalities through a web user in- terface optimized for swift responses and common failures while offering develop- ers and researchers a seamless deployment experience on local setups, providing a foundation for crafting innovative language agents and facilitating real-world evaluations. We elucidate the challenges and opportunities, aspiring to set a foun- dation for future research and development of real-world language agents.",
}